groups:
- name: k8s.rules
  rules:
  - expr: 'sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container_name!=""}[5m]))
      by (namespace)

      '
    record: namespace:container_cpu_usage_seconds_total:sum_rate
  - expr: "sum by (namespace, pod_name, container_name) (\n  rate(container_cpu_usage_seconds_total{job=\"\
      kubelet\", image!=\"\", container_name!=\"\"}[5m])\n)\n"
    record: namespace_pod_name_container_name:container_cpu_usage_seconds_total:sum_rate
  - expr: 'sum(container_memory_usage_bytes{job="kubelet", image!="", container_name!=""})
      by (namespace)

      '
    record: namespace:container_memory_usage_bytes:sum
  - expr: "sum by (namespace, label_name) (\n   sum(rate(container_cpu_usage_seconds_total{job=\"\
      kubelet\", image!=\"\", container_name!=\"\"}[5m])) by (namespace, pod_name)\n\
      \ * on (namespace, pod_name) group_left(label_name)\n   label_replace(kube_pod_labels{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}, \"pod_name\", \"$1\", \"\
      pod\", \"(.*)\")\n)\n"
    record: namespace_name:container_cpu_usage_seconds_total:sum_rate
  - expr: "sum by (namespace, label_name) (\n  sum(container_memory_usage_bytes{job=\"\
      kubelet\",image!=\"\", container_name!=\"\"}) by (pod_name, namespace)\n* on\
      \ (namespace, pod_name) group_left(label_name)\n  label_replace(kube_pod_labels{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}, \"pod_name\", \"$1\", \"\
      pod\", \"(.*)\")\n)\n"
    record: namespace_name:container_memory_usage_bytes:sum
  - expr: "sum by (namespace, label_name) (\n  sum(kube_pod_container_resource_requests_memory_bytes{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}) by (namespace, pod)\n* on\
      \ (namespace, pod) group_left(label_name)\n  label_replace(kube_pod_labels{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}, \"pod_name\", \"$1\", \"\
      pod\", \"(.*)\")\n)\n"
    record: namespace_name:kube_pod_container_resource_requests_memory_bytes:sum
  - expr: "sum by (namespace, label_name) (\n  sum(kube_pod_container_resource_requests_cpu_cores{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"} and on(pod) kube_pod_status_scheduled{condition=\"\
      true\"}) by (namespace, pod)\n* on (namespace, pod) group_left(label_name)\n\
      \  label_replace(kube_pod_labels{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }, \"pod_name\", \"$1\", \"pod\", \"(.*)\")\n)\n"
    record: namespace_name:kube_pod_container_resource_requests_cpu_cores:sum
- name: kube-scheduler.rules
  rules:
  - expr: 'histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.99'
    record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
  - expr: 'histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.99'
    record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
  - expr: 'histogram_quantile(0.99, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.99'
    record: cluster_quantile:scheduler_binding_latency:histogram_quantile
  - expr: 'histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.9'
    record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
  - expr: 'histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.9'
    record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
  - expr: 'histogram_quantile(0.9, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.9'
    record: cluster_quantile:scheduler_binding_latency:histogram_quantile
  - expr: 'histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.5'
    record: cluster_quantile:scheduler_e2e_scheduling_latency:histogram_quantile
  - expr: 'histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.5'
    record: cluster_quantile:scheduler_scheduling_algorithm_latency:histogram_quantile
  - expr: 'histogram_quantile(0.5, sum(rate(scheduler_binding_latency_microseconds_bucket{job="kube-scheduler"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.5'
    record: cluster_quantile:scheduler_binding_latency:histogram_quantile
- name: kube-apiserver.rules
  rules:
  - expr: 'histogram_quantile(0.99, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.99'
    record: cluster_quantile:apiserver_request_latencies:histogram_quantile
  - expr: 'histogram_quantile(0.9, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.9'
    record: cluster_quantile:apiserver_request_latencies:histogram_quantile
  - expr: 'histogram_quantile(0.5, sum(rate(apiserver_request_latencies_bucket{job="apiserver"}[5m]))
      without(instance, pod)) / 1e+06

      '
    labels:
      quantile: '0.5'
    record: cluster_quantile:apiserver_request_latencies:histogram_quantile
- name: node.rules
  rules:
  - expr: sum(min(kube_pod_info) by (node))
    record: ':kube_pod_info_node_count:'
  - expr: 'max(label_replace(kube_pod_info{job=~"kube-state-metrics|kubernetes-service-endpoints"},
      "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)

      '
    record: 'node_namespace_pod:kube_pod_info:'
  - expr: "count by (node) (sum by (node, cpu) (\n  node_cpu_seconds_total{job=\"\
      node-exporter\"}\n* on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n\
      ))\n"
    record: node:node_num_cpu:sum
  - expr: '1 - avg(rate(node_cpu_seconds_total{job="node-exporter",mode="idle"}[1m]))

      '
    record: :node_cpu_utilisation:avg1m
  - expr: "1 - avg by (node) (\n  rate(node_cpu_seconds_total{job=\"node-exporter\"\
      ,mode=\"idle\"}[1m])\n* on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:)\n"
    record: node:node_cpu_utilisation:avg1m
  - expr: 'sum(node_load1{job="node-exporter"})

      /

      sum(node:node_num_cpu:sum)

      '
    record: ':node_cpu_saturation_load1:'
  - expr: "sum by (node) (\n  node_load1{job=\"node-exporter\"}\n* on (namespace,\
      \ pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n)\n/\nnode:node_num_cpu:sum\n"
    record: 'node:node_cpu_saturation_load1:'
  - expr: '1 -

      sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"}
      + node_memory_Buffers_bytes{job="node-exporter"})

      /

      sum(node_memory_MemTotal_bytes{job="node-exporter"})

      '
    record: ':node_memory_utilisation:'
  - expr: 'sum(node_memory_MemFree_bytes{job="node-exporter"} + node_memory_Cached_bytes{job="node-exporter"}
      + node_memory_Buffers_bytes{job="node-exporter"})

      '
    record: :node_memory_MemFreeCachedBuffers_bytes:sum
  - expr: 'sum(node_memory_MemTotal_bytes{job="node-exporter"})

      '
    record: :node_memory_MemTotal_bytes:sum
  - expr: "sum by (node) (\n  (node_memory_MemFree_bytes{job=\"node-exporter\"} +\
      \ node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_Buffers_bytes{job=\"\
      node-exporter\"})\n  * on (namespace, pod) group_left(node)\n    node_namespace_pod:kube_pod_info:\n\
      )\n"
    record: node:node_memory_bytes_available:sum
  - expr: "sum by (node) (\n  node_memory_MemTotal_bytes{job=\"node-exporter\"}\n\
      \  * on (namespace, pod) group_left(node)\n    node_namespace_pod:kube_pod_info:\n\
      )\n"
    record: node:node_memory_bytes_total:sum
  - expr: '(node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)

      /

      scalar(sum(node:node_memory_bytes_total:sum))

      '
    record: node:node_memory_utilisation:ratio
  - expr: "1e3 * sum(\n  (rate(node_vmstat_pgpgin{job=\"node-exporter\"}[1m])\n +\
      \ rate(node_vmstat_pgpgout{job=\"node-exporter\"}[1m]))\n)\n"
    record: :node_memory_swap_io_bytes:sum_rate
  - expr: "1 -\nsum by (node) (\n  (node_memory_MemFree_bytes{job=\"node-exporter\"\
      } + node_memory_Cached_bytes{job=\"node-exporter\"} + node_memory_Buffers_bytes{job=\"\
      node-exporter\"})\n* on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n\
      )\n/\nsum by (node) (\n  node_memory_MemTotal_bytes{job=\"node-exporter\"}\n\
      * on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n\
      )\n"
    record: 'node:node_memory_utilisation:'
  - expr: '1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)

      '
    record: 'node:node_memory_utilisation_2:'
  - expr: "1e3 * sum by (node) (\n  (rate(node_vmstat_pgpgin{job=\"node-exporter\"\
      }[1m])\n + rate(node_vmstat_pgpgout{job=\"node-exporter\"}[1m]))\n * on (namespace,\
      \ pod) group_left(node)\n   node_namespace_pod:kube_pod_info:\n)\n"
    record: node:node_memory_swap_io_bytes:sum_rate
  - expr: 'avg(irate(node_disk_io_time_seconds_total{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m]))

      '
    record: :node_disk_utilisation:avg_irate
  - expr: "avg by (node) (\n  irate(node_disk_io_time_seconds_total{job=\"node-exporter\"\
      ,device=~\"(sd|xvd|nvme).+\"}[1m])\n* on (namespace, pod) group_left(node)\n\
      \  node_namespace_pod:kube_pod_info:\n)\n"
    record: node:node_disk_utilisation:avg_irate
  - expr: 'avg(irate(node_disk_io_time_weighted_seconds_total{job="node-exporter",device=~"(sd|xvd|nvme).+"}[1m])
      / 1e3)

      '
    record: :node_disk_saturation:avg_irate
  - expr: "avg by (node) (\n  irate(node_disk_io_time_weighted_seconds_total{job=\"\
      node-exporter\",device=~\"(sd|xvd|nvme).+\"}[1m]) / 1e3\n* on (namespace, pod)\
      \ group_left(node)\n  node_namespace_pod:kube_pod_info:\n)\n"
    record: node:node_disk_saturation:avg_irate
  - expr: 'max by (namespace, pod, device, instance) ((node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}

      - node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})

      / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})

      '
    record: 'node:node_filesystem_usage:'
  - expr: 'max by (namespace, pod, device) (node_filesystem_avail_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"}
      / node_filesystem_size_bytes{fstype=~"ext[234]|btrfs|xfs|zfs"})

      '
    record: 'node:node_filesystem_avail:'
  - expr: 'sum(irate(node_network_receive_bytes_total{job="node-exporter",device="eth0"}[1m]))
      +

      sum(irate(node_network_transmit_bytes_total{job="node-exporter",device="eth0"}[1m]))

      '
    record: :node_net_utilisation:sum_irate
  - expr: "sum by (node) (\n  (irate(node_network_receive_bytes_total{job=\"node-exporter\"\
      ,device=\"eth0\"}[1m]) +\n  irate(node_network_transmit_bytes_total{job=\"node-exporter\"\
      ,device=\"eth0\"}[1m]))\n* on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n\
      )\n"
    record: node:node_net_utilisation:sum_irate
  - expr: 'sum(irate(node_network_receive_drop_total{job="node-exporter",device="eth0"}[1m]))
      +

      sum(irate(node_network_transmit_drop_total{job="node-exporter",device="eth0"}[1m]))

      '
    record: :node_net_saturation:sum_irate
  - expr: "sum by (node) (\n  (irate(node_network_receive_drop_total{job=\"node-exporter\"\
      ,device=\"eth0\"}[1m]) +\n  irate(node_network_transmit_drop_total{job=\"node-exporter\"\
      ,device=\"eth0\"}[1m]))\n* on (namespace, pod) group_left(node)\n  node_namespace_pod:kube_pod_info:\n\
      )\n"
    record: node:node_net_saturation:sum_irate
- name: kube-prometheus-node-recording.rules
  rules:
  - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[3m])) BY (instance)
    record: instance:node_cpu:rate:sum
  - expr: sum((node_filesystem_size{mountpoint="/"} - node_filesystem_free{mountpoint="/"}))
      BY (instance)
    record: instance:node_filesystem_usage:sum
  - expr: sum(rate(node_network_receive_bytes[3m])) BY (instance)
    record: instance:node_network_receive_bytes:rate:sum
  - expr: sum(rate(node_network_transmit_bytes[3m])) BY (instance)
    record: instance:node_network_transmit_bytes:rate:sum
  - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m])) WITHOUT (cpu, mode)
      / ON(instance) GROUP_LEFT() count(sum(node_cpu) BY (instance, cpu)) BY (instance)
    record: instance:node_cpu:ratio
  - expr: sum(rate(node_cpu{mode!="idle",mode!="iowait"}[5m]))
    record: cluster:node_cpu:sum_rate5m
  - expr: cluster:node_cpu:rate5m / count(sum(node_cpu) BY (instance, cpu))
    record: cluster:node_cpu:ratio
- name: kubernetes-absent
  rules:
  - alert: KubeAPIDown
    annotations:
      description: KubeAPI has disappeared from Prometheus target discovery.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
    expr: 'absent(up{job="apiserver"} == 1)

      '
    for: 2m
    labels:
      app: kube-apiserver
      env: kube-system
      severity: critical
  - alert: KubeStateMetricsDown
    annotations:
      description: KubeStateMetrics has disappeared from Prometheus target discovery.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricsdown
    expr: 'absent(up{job=~"kube-state-metrics|kubernetes-service-endpoints"} == 1)

      '
    for: 5m
    labels:
      app: kube-state-metrics
      env: kube-system
      severity: critical
  - alert: NodeExporterDown
    annotations:
      description: NodeExporter has disappeared from Prometheus target discovery.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeexporterdown
    expr: 'absent(up{job="node-exporter"} == 1)

      '
    for: 5m
    labels:
      app: node-exporter
      env: monitoring
      severity: critical
  - alert: ClusterAutoscalerDown
    annotations:
      description: Cluster autoscaler has disappeared from Prometheus target discovery.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerdown
    expr: 'absent(up{job="cluster-autoscaler"} == 1)

      '
    for: 2m
    labels:
      app: cluster-autoscaler
      env: kube-system
      severity: critical
  - alert: CoreDNSDown
    annotations:
      description: Core DNS has disappeared from Prometheus target discovery.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerdown
    expr: 'absent(up{job="coredns"} == 1)

      '
    for: 2m
    labels:
      app: coredns
      env: kube-system
      severity: critical
- name: kubernetes-apps
  rules:
  - alert: MaxReplicaReachedForAppCritical
    annotations:
      description: App {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler
        }} has reached 90 percent of max replica for more than 10m in cluster {{ $labels.cluster
        }}.
    expr: kube_horizontalpodautoscaler_status_current_replicas{hpa!="istio-pilot",cluster!="quartz-d"}/kube_horizontalpodautoscaler_spec_max_replicas{cluster!="quartz-d"}*100
      > 90 and  (kube_horizontalpodautoscaler_spec_max_replicas{cluster!="quartz-d"}!=kube_horizontalpodautoscaler_spec_min_replicas{cluster!="quartz-d"})
    for: 20m
    labels:
      app: '{{ $labels.horizontalpodautoscaler }}'
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: MaxReplicaReachedForAppWarning
    annotations:
      description: App {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler
        }} has reached 80 percent of max replica for more than 10m in cluster {{ $labels.cluster
        }}.
    expr: kube_horizontalpodautoscaler_status_current_replicas{hpa!="istio-pilot",cluster!="quartz-d"}/kube_horizontalpodautoscaler_spec_max_replicas{cluster!="quartz-d"}*100
      > 80 and  (kube_horizontalpodautoscaler_spec_max_replicas{cluster!="quartz-d"}!=kube_horizontalpodautoscaler_spec_min_replicas{cluster!="quartz-d"})
    for: 20m
    labels:
      app: '{{ $labels.horizontalpodautoscaler }}'
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: DeployedAppInstanceCrashLooping
    annotations:
      description: Instance {{ $labels.namespace }}/{{ $labels.pod }} of ({{ $labels.container
        }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
    expr: 'rate(kube_pod_container_status_restarts_total{job=~"kube-state-metrics|kubernetes-service-endpoints",
      service!~"seo-web-food-staging|unboxing-web-staging|checkout-web-app-staging"}[15m])
      * 60 * 5 > 1

      '
    for: 2m
    labels:
      app: '{{ $labels.container }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: DeployedAppInstanceRestarted
    annotations:
      description: Instance {{ $labels.namespace }}/{{ $labels.pod }} of ({{ $labels.container
        }})  restarted.
    expr: '(kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total
      offset 3m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason
      != ""}[3m]) == 1

      '
    for: 2m
    labels:
      app: '{{ $labels.container }}'
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: DeployedAppNotReadyWarning
    annotations:
      description: Instance {{ $labels.namespace }}/{{ $labels.pod }} on cluster {{
        $labels.cluster }} has been in pending state for longer than 5 minutes.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready
    expr: 'sum by (namespace, pod,cluster) (kube_pod_status_phase{job=~"kube-state-metrics|kubernetes-service-endpoints",
      phase=~"Pending|Unknown",pod!~"cluster-autoscaler-pause.*"}) > 0

      '
    for: 20m
    labels:
      app: '{{ $labels.pod }}'
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: DeployedAppNotReadyCritical
    annotations:
      description: Instance {{ $labels.namespace }}/{{ $labels.pod }} on cluster {{
        $labels.cluster }} has been in pending state for longer than 10 minutes.
    expr: 'sum by (namespace, pod,cluster) (kube_pod_status_phase{job=~"kube-state-metrics|kubernetes-service-endpoints",
      phase=~"Pending|Unknown",pod!~"cluster-autoscaler-pause.*"}) > 0

      '
    for: 30m
    labels:
      app: rock
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: DeploymentGenerationMismatch
    annotations:
      description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
        }} does not match, this indicates that the Deployment has failed but has not
        been rolled back.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch
    expr: "kube_deployment_status_observed_generation{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n  !=\nkube_deployment_metadata_generation{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n"
    for: 5m
    labels:
      app: '{{ $labels.deployment }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: DeploymentReplicasMismatch
    annotations:
      description: App Deployment {{ $labels.namespace }}/{{ $labels.deployment }}
        has not matched the expected number of instances for longer than 25 minutes.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch
    expr: "kube_deployment_spec_replicas{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n  !=\nkube_deployment_status_replicas_available{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n"
    for: 25m
    labels:
      app: rock
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: StatefulSetReplicasMismatch
    annotations:
      description: App Deployment with Volume {{ $labels.namespace }}/{{ $labels.statefulset
        }} has not matched the expected number of replicas for longer than 15 minutes.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch
    expr: "kube_statefulset_status_replicas_ready{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n  !=\nkube_statefulset_status_replicas{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n"
    for: 15m
    labels:
      app: '{{ $labels.statefulset }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: StatefulSetGenerationMismatch
    annotations:
      description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
        }} does not match, this indicates that the StatefulSet has failed but has
        not been rolled back.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch
    expr: "kube_statefulset_status_observed_generation{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n  !=\nkube_statefulset_metadata_generation{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n"
    for: 15m
    labels:
      app: '{{ $labels.statefulset }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: KubeStatefulSetUpdateNotRolledOut
    annotations:
      description: Stateful App {{ $labels.namespace }}/{{ $labels.statefulset }}
        update has not been rolled out.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetupdatenotrolledout
    expr: "max without (revision) (\n  kube_statefulset_status_current_revision{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}\n    unless\n  kube_statefulset_status_update_revision{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}\n)\n  *\n(\n  kube_statefulset_replicas{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}\n    !=\n  kube_statefulset_status_replicas_updated{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\"}\n)\n"
    for: 15m
    labels:
      app: '{{ $labels.statefulset }}'
      severity: critical
  - alert: KubeDaemonSetRolloutStuck
    annotations:
      app: '{{ $labels.daemonset }}'
      description: Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
        }}/{{ $labels.daemonset }} are scheduled and ready.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck
    expr: "kube_daemonset_status_number_ready{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n  /\nkube_daemonset_status_desired_number_scheduled{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      } * 100 < 100\n"
    for: 15m
    labels:
      pod: devops
      severity: warning
  - alert: KubeDaemonSetNotScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are not scheduled.'
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled
    expr: "kube_daemonset_status_desired_number_scheduled{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      }\n  -\nkube_daemonset_status_current_number_scheduled{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      } > 0\n"
    for: 10m
    labels:
      app: '{{ $labels.daemonset }}'
      env: '{{ $labels.namespace }}'
      pod: devops
      severity: warning
  - alert: KubeDaemonSetMisScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
        }} are running where they are not supposed to run.'
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled
    expr: 'kube_daemonset_status_number_misscheduled{job=~"kube-state-metrics|kubernetes-service-endpoints"}
      > 2

      '
    for: 10m
    labels:
      app: '{{ $labels.daemonset }}'
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: KubeCronJobRunning
    annotations:
      description: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking
        more than 1h to complete.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning
    expr: 'time() - kube_cronjob_next_schedule_time{job=~"kube-state-metrics|kubernetes-service-endpoints"}
      > 3600

      '
    for: 1h
    labels:
      app: '{{ $labels.cronjob }}'
      env: '{{ $labels.namespace }}'
      severity: warning
  - alert: KubeJobCompletion
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
        than one hour to complete.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion
    expr: 'kube_job_spec_completions{job=~"kube-state-metrics|kubernetes-service-endpoints"}
      - kube_job_status_succeeded{job=~"kube-state-metrics|kubernetes-service-endpoints"}  >
      0

      '
    for: 1h
    labels:
      app: '{{ $labels.job_name }}'
      env: '{{ $labels.namespace }}'
      pod: devops
      severity: warning
  - alert: AppCPUUsageCritical
    annotations:
      description: App instance {{ $labels.pod_name }} in {{ $labels.namespace }}
        cpu usage more than 80%.
      runbook_url: https://grafana.production.singapore.swiggy.cloud/d/mqiIX3_ik/individual-app-dashboard?orgId=1
    expr: '((sum(rate(container_cpu_usage_seconds_total{container_name!="",container_name!~"POD|istio-proxy|catalog-intelligence|swiggylistingserviceratelimitersidecar|vidura-runtime-dash",image!="",job="kubelet"}[3m]))
      by (container_name,namespace,pod_name)) / on(pod_name, container_name, namespace)
      (sum(container_spec_cpu_shares{ namespace=~"production|preprod",container_name!~"POD|istio-proxy|catalog-intelligence|swiggylistingserviceratelimitersidecar|vidura-runtime-dash",image!="",job="kubelet"}/1024)
      by (container_name,namespace,pod_name)))*100 > 80

      '
    for: 10m
    labels:
      app: '{{ $labels.container_name }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: AppCPUUsageCriticalDash
    annotations:
      description: App instance {{ $labels.pod_name }} in {{ $labels.namespace }}
        cpu usage more than 80%.
      runbook_url: https://grafana.production.singapore.swiggy.cloud/d/mqiIX3_ik/individual-app-dashboard?orgId=1
    expr: '((sum(rate(container_cpu_usage_seconds_total{container_name!="",container_name=~"vidura-runtime-dash",image!="",job="kubelet"}[3m]))
      by (container_name,namespace,pod_name)) / on(pod_name, container_name, namespace)
      (sum(container_spec_cpu_shares{ namespace=~"production|preprod",container_name=~"vidura-runtime-dash",image!="",job="kubelet"}/1024)
      by (container_name,namespace,pod_name)))*100 > 80

      '
    for: 15m
    labels:
      app: vidura-runtime-dash
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: AppMemoryUsageCritical
    annotations:
      description: App instance {{ $labels.pod_name }} in {{ $labels.namespace }}
        memory usage more than 80%.
      runbook_url: https://grafana.production.singapore.swiggy.cloud/d/mqiIX3_ik/individual-app-dashboard?orgId=1
    expr: '(container_memory_working_set_bytes{job="kubelet", image!="",container_name!="POD",container_name!=""}
      / (container_spec_memory_limit_bytes{job="kubelet", image!="",container_name!="POD",container_name!="",container_name!="aws-otel-collector"}
      > 0))*100 > 80

      '
    for: 10m
    labels:
      app: '{{ $labels.container_name }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: KubeJobFailed
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed
    expr: 'kube_job_status_failed{job=~"kube-state-metrics|kubernetes-service-endpoints"}  >
      0

      '
    for: 15m
    labels:
      app: '{{ $labels.job_name }}'
      env: '{{ $labels.namespace }}'
      pod: devops
      severity: warning
- name: kubernetes-resources
  rules:
  - alert: KubeCPUOvercommit
    annotations:
      description: Cluster has overcommitted CPU resource requests for Pods and cannot
        tolerate node failure.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
    expr: "sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n\
      \  /\nsum(node:node_num_cpu:sum)\n  >\n(count(node:node_num_cpu:sum)-1) / count(node:node_num_cpu:sum)\n"
    for: 5m
    labels:
      app: rock
      env: kube-system
      pod: devops
      severity: warning
  - alert: KubeMemOvercommit
    annotations:
      description: Cluster has overcommitted memory resource requests for Pods and
        cannot tolerate node failure.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
    expr: "sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n\
      \  /\nsum(node_memory_MemTotal_bytes)\n  >\n(count(node:node_num_cpu:sum)-1)\n\
      \  /\ncount(node:node_num_cpu:sum)\n"
    for: 5m
    labels:
      app: rock
      env: kube-system
      pod: devops
      severity: critical
  - alert: KubeCPUOvercommit
    annotations:
      description: Cluster has overcommitted CPU resource requests for Namespaces.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit
    expr: "sum(kube_resourcequota{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      , type=\"hard\", resource=\"requests.cpu\"})\n  /\nsum(node:node_num_cpu:sum)\n\
      \  > 1.5\n"
    for: 5m
    labels:
      pod: devops
      severity: warning
  - alert: KubeMemOvercommit
    annotations:
      description: Cluster has overcommitted memory resource requests for Namespaces.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit
    expr: "sum(kube_resourcequota{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      , type=\"hard\", resource=\"requests.memory\"})\n  /\nsum(node_memory_MemTotal_bytes{job=\"\
      node-exporter\"})\n  > 1.5\n"
    for: 5m
    labels:
      app: rock
      env: kube-system
      pod: devops
      severity: warning
  - alert: KubeQuotaExceeded
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value
        }}% of its {{ $labels.resource }} quota.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
    expr: "100 * kube_resourcequota{job=~\"kube-state-metrics|kubernetes-service-endpoints\"\
      , type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{job=~\"\
      kube-state-metrics|kubernetes-service-endpoints\", type=\"hard\"} > 0)\n  >\
      \ 90\n"
    for: 15m
    labels:
      app: rock
      env: kube-system
      pod: devops
      severity: warning
  - alert: CPUThrottlingHigh
    annotations:
      description: '{{ printf "%0.0f" $value }}% throttling of CPU in environment
        {{ $labels.namespace }} for container {{ $labels.container_name }} in pod
        {{ $labels.pod_name }}.'
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh
    expr: "100 * sum(increase(container_cpu_cfs_throttled_periods_total[5m])) by (container_name,\
      \ pod_name, namespace) \n  / \nsum(increase(container_cpu_cfs_periods_total[5m]))\
      \ by (container_name, pod_name, namespace)\n  > 25 \n"
    for: 15m
    labels:
      app: '{{ $labels.container_name }}'
      env: '{{ $labels.namespace }}'
      severity: warning
- name: kubernetes-storage
  rules:
  - alert: KubePersistentVolumeUsageCritical
    annotations:
      description: The PersistentVolume {{ $labels.persistentvolumeclaim }} claimed
        by {{ $labels.label_app }} in environment {{ $labels.namespace }} is only
        {{ printf "%0.2f" $value }}% free.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical
    expr: "100 * kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n  /\nkubelet_volume_stats_capacity_bytes{job=\"\
      kubelet\"} * on (namespace, persistentvolumeclaim) group_left(label_app) kube_persistentvolumeclaim_labels\n\
      \  < 20\n"
    for: 5m
    labels:
      app: '{{ $labels.label_app }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: KubePersistentVolumeFullInFourDays
    annotations:
      description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
        }} in environment {{ $labels.namespace }} is expected to fill up within four
        days. Currently {{ printf "%0.2f" $value }}% is available.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays
    expr: "100 * (\n  kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n    /\n\
      \  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n)  * on (namespace,\
      \ persistentvolumeclaim) group_left(label_app) kube_persistentvolumeclaim_labels\
      \ < 15\nand\npredict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"\
      }[6h], 4 * 24 * 3600) < 0\n"
    for: 1h
    labels:
      app: '{{ $labels.label_app }}'
      env: '{{ $labels.namespace }}'
      severity: critical
  - alert: KubePersistentVolumeErrors
    annotations:
      description: The persistent volume {{ $labels.persistentvolume }} has status
        {{ $labels.phase }}.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors
    expr: 'kube_persistentvolume_status_phase{phase=~"Failed|Pending",job=~"kube-state-metrics|kubernetes-service-endpoints"}  >
      0

      '
    for: 5m
    labels:
      app: infra-volume
      env: '{{ $labels.namespace }}'
      severity: critical
- name: kubernetes-system
  rules:
  - alert: KubeNodeNotReady
    annotations:
      description: '{{ $labels.node }} has been unready for more than 10 minutes.'
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
    expr: 'kube_node_status_condition{job=~"kube-state-metrics|kubernetes-service-endpoints",condition="Ready",status="true"}
      == 0

      '
    for: 10m
    labels:
      app: kube-node
      env: rock
      severity: warning
  - alert: KubeNodeCriticalCondition
    annotations:
      description: '{{ $labels.node }} has been unready for more than 10 minutes.'
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
    expr: 'kube_node_status_condition{job=~"kube-state-metrics|kubernetes-service-endpoints",condition!="Ready",status="true"}
      == 1

      '
    for: 2m
    labels:
      app: kube-node
      env: rock
      severity: warning
  - alert: KubeVersionMismatch
    annotations:
      description: There are {{ $value }} different versions of Kubernetes components
        running.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch
    expr: 'count(count(kubernetes_build_info{job!="kube-dns"}) by (gitVersion)) >
      1

      '
    for: 1h
    labels:
      app: kube-node
      env: kube-system
      severity: warning
  - alert: KubeClientErrors
    annotations:
      description: Kubernetes API server client {{ $labels.job }}/{{ $labels.instance
        }} is experiencing {{ printf "%0.0f" $value }}% errors.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
    expr: "(sum(rate(rest_client_requests_total{code!~\"2..|404\"}[5m])) by (instance,\
      \ job)\n  /\nsum(rate(rest_client_requests_total[5m])) by (instance, job))\n\
      * 100 > 1\n"
    for: 15m
    labels:
      app: kube-client
      env: kube-system
      pod: devops
      severity: warning
  - alert: KubeClientErrors
    annotations:
      description: Kubernetes API server client {{ $labels.job }}/{{ $labels.instance
        }} is experiencing {{ printf "%0.0f" $value }} errors / second.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors
    expr: 'sum(rate(ksm_scrape_error_total{job=~"kube-state-metrics|kubernetes-service-endpoints"}[5m]))
      by (instance, job) > 0.1

      '
    for: 15m
    labels:
      app: kube-client
      env: kube-system
      pod: devops
      severity: warning
  - alert: KubeAPILatencyHigh
    annotations:
      description: The API server has a 99th percentile latency of {{ $value }} seconds
        for {{ $labels.verb }} {{ $labels.resource }}.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
    expr: 'cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"}
      > 1

      '
    for: 10m
    labels:
      app: kube-api
      env: kube-system
      severity: warning
  - alert: KubeAPILatencyHigh
    annotations:
      description: The API server has a 99th percentile latency of {{ $value }} seconds
        for {{ $labels.verb }} {{ $labels.resource }}.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
    expr: 'cluster_quantile:apiserver_request_latencies:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"}
      > 4

      '
    for: 10m
    labels:
      app: kube-api
      env: kube-system
      severity: critical
  - alert: KubeAPIErrorsHigh
    annotations:
      description: API server is returning errors for {{ $value }}% of requests.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
    expr: sum by (resource,subresource,verb) (rate(apiserver_request_count{code=~"^(?:5..)$",job="apiserver",subresource=~".+"}[5m]))
      / sum by (resource,subresource,verb) (rate(apiserver_request_count{job="apiserver",subresource=~".+"}[5m]))
      * 100 > 2
    for: 10m
    labels:
      app: kube-api
      env: kube-system
      severity: critical
  - alert: KubeAPIErrorsHigh
    annotations:
      description: API server is returning errors for {{ $value }}% of requests.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
    expr: sum by (resource,subresource,verb) (rate(apiserver_request_count{code=~"^(?:5..)$",job="apiserver",subresource=~".+"}[5m]))
      / sum by (resource,subresource,verb) (rate(apiserver_request_count{job="apiserver",subresource=~".+"}[5m]))
      * 100 > 1
    for: 10m
    labels:
      app: kube-api
      env: kube-system
      severity: warning
  - alert: KubeClientCertificateExpiration
    annotations:
      description: Kubernetes API certificate is expiring in less than 7 days.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
    expr: 'histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
      < 604800

      '
    labels:
      app: kube-api
      env: kube-system
      severity: critical
  - alert: KubeClientCertificateExpiration
    annotations:
      description: Kubernetes API certificate is expiring in less than 24 hours.
      runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
    expr: 'histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m])))
      < 86400

      '
    labels:
      app: kube-api
      env: kube-system
      severity: critical
- name: alertmanager.rules
  rules:
  - alert: AlertmanagerConfigInconsistent
    annotations:
      description: The configuration of the instances of the Alertmanager cluster
        `{{$labels.service}}` are out of sync.
    expr: 'count_values("config_hash", alertmanager_config_hash{job="alertmanager-main"})
      BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_spec_replicas{job="prometheus-operator",controller="alertmanager"},
      "service", "alertmanager-$1", "name", "(.*)") != 1

      '
    for: 5m
    labels:
      app: alertmanager
      env: monitoring
      severity: critical
  - alert: AlertmanagerFailedReload
    annotations:
      description: Reloading Alertmanagers configuration has failed for {{ $labels.namespace
        }}/{{ $labels.pod}}.
    expr: 'alertmanager_config_last_reload_successful{job="alertmanager-main"} ==
      0

      '
    for: 10m
    labels:
      app: alertmanager
      env: monitoring
      severity: warning
  - alert: AlertmanagerMembersInconsistent
    annotations:
      description: Alertmanager has not found all other members of the cluster.
    expr: "alertmanager_cluster_members{job=\"alertmanager-main\"}\n  != on (service)\n\
      count by (service) (alertmanager_cluster_members{job=\"alertmanager-main\"})\n"
    for: 5m
    labels:
      app: alertmanager
      env: monitoring
      severity: critical
- name: general.rules
  rules:
  - alert: DeadMansSwitch
    annotations:
      description: Monitoring is up as of hour {{ $value }}
    expr: floor((hour()*60 +minute()+330)/60)
    labels:
      app: prometheus
      env: monitoring
      severity: none
- name: kube-prometheus-node-alerting.rules
  rules:
  - alert: NodeDiskRunningFull
    annotations:
      description: Device {{ $labels.device }} of node-exporter {{ $labels.namespace
        }}/{{ $labels.pod }} will be full within the next 24 hours.
    expr: '(node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h],
      3600 * 24) < 0)

      '
    for: 30m
    labels:
      app: prometheus
      env: monitoring
      severity: critical
  - alert: NodeDiskRunningFull
    annotations:
      description: Device {{ $labels.device }} of node-exporter {{ $labels.namespace
        }}/{{ $labels.pod }} will be full within the next 2 hours.
    expr: '(node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[30m],
      3600 * 2) < 0)

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: critical
- name: prometheus.rules
  rules:
  - alert: PrometheusConfigReloadFailed
    annotations:
      description: Reloading Prometheus configuration has failed for {{$labels.namespace}}/{{$labels.pod}}
      summary: Reloading Prometheus configuration failed
    expr: 'prometheus_config_last_reload_successful{job="prometheus-k8s"} == 0

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusNotificationQueueRunningFull
    annotations:
      description: Prometheus alert notification queue is running full for {{$labels.namespace}}/{{
        $labels.pod}}
      summary: Prometheus alert notification queue is running full
    expr: 'predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s"}[5m],
      60 * 30) > prometheus_notifications_queue_capacity{job="prometheus-k8s"}

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusErrorSendingAlerts
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
        $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
      summary: Errors while sending alert from Prometheus
    expr: 'rate(prometheus_notifications_errors_total{job="prometheus-k8s"}[5m]) /
      rate(prometheus_notifications_sent_total{job="prometheus-k8s"}[5m]) > 0.01

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusErrorSendingAlerts
    annotations:
      description: Errors while sending alerts from Prometheus {{$labels.namespace}}/{{
        $labels.pod}} to Alertmanager {{$labels.Alertmanager}}
      summary: Errors while sending alerts from Prometheus
    expr: 'rate(prometheus_notifications_errors_total{job="prometheus-k8s"}[5m]) /
      rate(prometheus_notifications_sent_total{job="prometheus-k8s"}[5m]) > 0.03

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: critical
  - alert: PrometheusNotConnectedToAlertmanagers
    annotations:
      description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} is not connected
        to any Alertmanagers
      summary: Prometheus is not connected to any Alertmanagers
    expr: 'prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s"}
      < 1

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusTSDBReloadsFailing
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
        reload failures over the last four hours.'
      summary: Prometheus has issues reloading data blocks from disk
    expr: 'increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s"}[2h])
      > 0

      '
    for: 12h
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusTSDBCompactionsFailing
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}}
        compaction failures over the last four hours.'
      summary: Prometheus has issues compacting sample blocks
    expr: 'increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s"}[2h])
      > 0

      '
    for: 12h
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusTSDBWALCorruptions
    annotations:
      description: '{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead
        log (WAL).'
      summary: Prometheus write-ahead log is corrupted
    expr: 'tsdb_wal_corruptions_total{job="prometheus-k8s"} > 0

      '
    for: 4h
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusNotIngestingSamples
    annotations:
      description: Prometheus {{ $labels.namespace }}/{{ $labels.pod}} isnt ingesting
        samples.
      summary: Prometheus isnt ingesting samples
    expr: 'rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s"}[5m])
      <= 0

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusTargetScrapesDuplicate
    annotations:
      description: '{{$labels.namespace}}/{{$labels.pod}} has many samples rejected
        due to duplicate timestamps but different values'
      summary: Prometheus has many samples rejected
    expr: 'increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s"}[5m])
      > 0

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
- name: prometheus-operator
  rules:
  - alert: PrometheusOperatorReconcileErrors
    annotations:
      description: Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace
        }} Namespace.
    expr: 'rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator"}[5m])
      > 0.1

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusOperatorNodeLookupErrors
    annotations:
      description: Errors while reconciling Prometheus in {{ $labels.namespace }}
        Namespace.
    expr: 'rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator"}[5m])
      > 0.1

      '
    for: 10m
    labels:
      app: prometheus
      env: monitoring
      severity: warning
  - alert: PrometheusScrapeLimitsCritical
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . Metrics will be dropped post 10k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service!~"aniket-service|aniket-service-canary|collections-service|collections-service-canary|vidura-runtime-ads|vidura-runtime-perf|vidura-runtime-delivery-batch|vidura-runtime-listing-new|vidura-runtime-listing-new-canary|ff-reminder-notifier|ff-reminder-notifier-canary|vidura-runtime-listing|auto-assign|hulk-consumer|ie-pushgateway|cdc-prod-pg-cdc-prod-pg|sentinel|cdc-snowflake-sync|dash-business-metrics-service|dash-data-service|dash-picker-front|dash-cart|auto-assign-canary|hulk-consumer-canary|ie-pushgateway-canary|cdc-prod-pg-cdc-prod-pg-canary|sentinel-canary|cdc-snowflake-sync-canary|dash-business-metrics-service-canary|dash-data-service-canary|dash-picker-front-canary|dash-cart-canary|surff|surff-canary|widget-manager|widget-manager-canary|cerebro|cerebro-canary|core-discounting-server|core-discounting-server-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|one-compass-service|one-compass-service-canary|food-pre-delivery-bot|food-pre-delivery-bot-canary|vidura-runtime-delivery-batch-canary|vidura-runtime-ads-canary|preorder-cod-eval|preorder-cod-eval-canary|dash-serviceability|dash-serviceability-canary|offer-server|offer-server-canary|im-checkout-service|im-checkout-service-canary|vidura-runtime-dash|vidura-runtime-dash-canary|del-banner-go|del-banner-go-canary|unboxing-service|unboxing-service-canary|svcplatform-prediction|svcplatform-prediction-canary|svc-prediction|svc-prediction-canary|preorder-cod-eval|preorder-cod-eval-canary|vidura-runtime-ads|vidura-runtime-ads-canary|vidura-runtime-menu|vidura-runtime-menu-canary|de-lifecycle-management|de-lifecycle-management-canary|seo-data|seo-data-canary|picker-assignment|picker-assignment-canary|postorder-abuse-eval|postorder-abuse-eval-canary|svcplatform-aggregator|svcplatform-aggregator-canary|order-platform|order-platform-canary|carousel-service-nav|carousel-service-nav-canary|carousel-service|carousel-service-canary|de-allocator|de-allocator-canary|checkout-service|confluent-lag-exporter|cerebro-listing|ads-serving-layer-rest|ads-serving-layer|ingestion-pipeline|banner|confluent-lag-exporter-canary|cerebro-listing-canary|ads-serving-layer-rest-canary|ads-serving-layer-canary|ingestion-pipeline-canary|banner-canary|del-banner-go|del-banner-go-canary|cerebro|carousel-service|carousel-service-canary|gandalf|gandalf-canary|collections-service|collections-service-canary|carousel-service-nav|carousel-service-nav-canary|dash-business-metrics-service|dash-business-metrics-service-canary|test-service|test-service-canary"})
      > 9000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsWarning
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}.
        Metrics will be dropped post 10k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service!~"aniket-service|aniket-service-canary|collections-service|collections-service-canary|vidura-runtime-ads|vidura-runtime-perf|vidura-runtime-delivery-batch|vidura-runtime-listing-new|vidura-runtime-listing-new-canary|ff-reminder-notifier|ff-reminder-notifier-canary|vidura-runtime-listing|auto-assign|hulk-consumer|ie-pushgateway|cdc-prod-pg-cdc-prod-pg|sentinel|cdc-snowflake-sync|dash-business-metrics-service|dash-data-service|dash-picker-front|dash-cart|auto-assign-canary|hulk-consumer-canary|ie-pushgateway-canary|cdc-prod-pg-cdc-prod-pg-canary|sentinel-canary|cdc-snowflake-sync-canary|dash-business-metrics-service-canary|dash-data-service-canary|dash-picker-front-canary|dash-cart-canary|surff|surff-canary|widget-manager|widget-manager-canary|cerebro|cerebro-canary|core-discounting-server|core-discounting-server-canary|preorder-cod-eval|preorder-cod-eval-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|one-compass-service|one-compass-service-canary|food-pre-delivery-bot|food-pre-delivery-bot-canary|dash-serviceability|dash-serviceability-canary|offer-server|offer-server-canary|im-checkout-service|im-checkout-service-canary|vidura-runtime-dash|vidura-runtime-dash-canary|del-banner-go|del-banner-go-canary|unboxing-service|unboxing-service-canary|svcplatform-prediction|svcplatform-prediction-canary|svc-prediction|svc-prediction-canary|preorder-cod-eval|preorder-cod-eval-canary|vidura-runtime-ads|vidura-runtime-ads-canary|vidura-runtime-menu|vidura-runtime-menu-canary|de-lifecycle-management|de-lifecycle-management-canary|seo-data|seo-data-canary|picker-assignment|picker-assignment-canary|janus|postorder-abuse-eval|postorder-abuse-eval-canary|svcplatform-aggregator|svcplatform-aggregator-canary|order-platform|order-platform-canary|carousel-service-nav|carousel-service-nav-canary|carousel-service|carousel-service-canary|de-allocator|de-allocator-canary|checkout-service|confluent-lag-exporter|cerebro-listing|ads-serving-layer-rest|ads-serving-layer|ingestion-pipeline|banner|confluent-lag-exporter-canary|cerebro-listing-canary|ads-serving-layer-rest-canary|ads-serving-layer-canary|ingestion-pipeline-canary|banner-canary|del-banner-go|del-banner-go-canary|cerebro|carousel-service|carousel-service-canary|gandalf|gandalf-canary|collections-service|collections-service-canary|carousel-service-nav|carousel-service-nav-canary|dash-business-metrics-service|dash-business-metrics-service-canary|test-service|test-service-canary"})
      > 8000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: warning
  - alert: PrometheusScrapeLimitsWarning
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}.
        Metrics will be dropped post 10k sample count.
    expr: sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service="janus"})
      > 9500
    for: 15m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: warning
  - alert: PrometheusMetricsDropped
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . The samples will be dropped by Prometheus as it has breached 10k sample
        limit.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service!~"aniket-service|aniket-service-canary|collections-service|collections-service-canary|vidura-runtime-ads|vidura-runtime-ads-canary|vidura-runtime-perf|vidura-runtime-delivery-batch|vidura-runtime-listing-new|vidura-runtime-listing-new-canary|ff-reminder-notifier|ff-reminder-notifier-canary|vidura-runtime-listing|auto-assign|hulk-consumer|ie-pushgateway|cdc-prod-pg-cdc-prod-pg|sentinel|cdc-snowflake-sync|dash-business-metrics-service|dash-data-service|dash-picker-front|dash-cart|auto-assign-canary|hulk-consumer-canary|ie-pushgateway-canary|cdc-prod-pg-cdc-prod-pg-canary|sentinel-canary|cdc-snowflake-sync-canary|dash-business-metrics-service-canary|dash-data-service-canary|dash-picker-front-canary|dash-cart-canary|surff|surff-canary|widget-manager|widget-manager-canary|cerebro|cerebro-canary|core-discounting-server|core-discounting-server-canary|ads-serving-layer-rest|ads-serving-layer-rest|ads-serving-layer|ads-serving-layer-canary|one-compass-service|one-compass-service-canary|food-pre-delivery-bot|food-pre-delivery-bot-canary|dash-serviceability|dash-serviceability-canary|offer-server|offer-server-canary|im-checkout-service|im-checkout-service-canary|vidura-runtime-dash|vidura-runtime-dash-canary|del-banner-go|del-banner-go-canary|unboxing-service|unboxing-service-canary|svcplatform-prediction|svcplatform-prediction-canary|svc-prediction|svc-prediction-canary|preorder-cod-eval|preorder-cod-eval-canary|vidura-runtime-ads|vidura-runtime-ads-canary|vidura-runtime-menu|vidura-runtime-menu-canary|de-lifecycle-management|de-lifecycle-management-canary|seo-data|seo-data-canary|picker-assignment|picker-assignment-canary|postorder-abuse-eval|postorder-abuse-eval-canary|svcplatform-aggregator|svcplatform-aggregator-canary|order-platform|order-platform-canary|carousel-service-nav|carousel-service-nav-canary|carousel-service|carousel-service-canary|de-allocator|de-allocator-canary|checkout-service|confluent-lag-exporter|cerebro-listing|ads-serving-layer-rest|ads-serving-layer|ingestion-pipeline|banner|confluent-lag-exporter-canary|cerebro-listing-canary|ads-serving-layer-rest-canary|ads-serving-layer-canary|ingestion-pipeline-canary|banner-canary|del-banner-go|del-banner-go-canary|cerebro|carousel-service|carousel-service-canary|gandalf|gandalf-canary|collections-service|collections-service-canary|carousel-service-nav|carousel-service-nav-canary|dash-business-metrics-service|dash-business-metrics-service-canary|test-service|test-service-canary"})
      > 10000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsCritical
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . Metrics will be dropped post 15k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"vidura-runtime-delivery-batch|ads-serving-layer|ads-serving-layer-canary|vidura-runtime-delivery-batch-canary|vidura-runtime-ads-canary|dash-serviceability|dash-serviceability-canary"})
      > 14000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsWarning
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}.
        Metrics will be dropped post 15k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"vidura-runtime-delivery-batch|ads-serving-layer|ads-serving-layer-canary|dash-serviceability|dash-serviceability-canary"})
      > 13000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: warning
  - alert: PrometheusMetricsDropped
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . The samples will be dropped by Prometheus as it has breached 15k sample
        limit.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"vidura-runtime-delivery-batch|ads-serving-layer-canary|ads-serving-layer|dash-serviceability|dash-serviceability-canary"})
      > 15000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsCritical
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . Metrics will be dropped post 20k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"vidura-runtime-perf|vidura-runtime-listing-new|vidura-runtime-listing-new-canary|ff-reminder-notifier|ff-reminder-notifier-canary|vidura-runtime-listing|hulk-consumer|sentinel|dash-data-service|dash-picker-front|dash-cart|hulk-consumer-canary|sentinel-canary|dash-data-service-canary|dash-picker-front-canary|dash-cart-canary|surff|surff-canary|widget-manager|widget-manager-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|one-compass-service|one-compass-service-canary|food-pre-delivery-bot|food-pre-delivery-bot-canary|dash-serviceability|dash-serviceability-canary|offer-server|offer-server-canary|im-checkout-service|im-checkout-service-canary|vidura-runtime-dash|vidura-runtime-dash-canary|del-banner-go|del-banner-go-canary|unboxing-service|unboxing-service-canary|svcplatform-prediction|svcplatform-prediction-canary|svc-prediction|svc-prediction-canary|preorder-cod-eval|preorder-cod-eval-canary|preorder-cod-eval|preorder-cod-eval-canary|vidura-runtime-menu|vidura-runtime-menu-canary|de-lifecycle-management|de-lifecycle-management-canary|seo-data|seo-data-canary|picker-assignment|picker-assignment-canary|postorder-abuse-eval|postorder-abuse-eval-canary|svcplatform-aggregator|svcplatform-aggregator-canary|order-platform|order-platform-canary|carousel-service|carousel-service-canary|aniket-service|aniket-service-canary|test-service|test-service-canary"})
      > 19000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsWarning
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}.
        Metrics will be dropped post 20k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"vidura-runtime-perf|vidura-runtime-listing-new|vidura-runtime-listing-new-canary|ff-reminder-notifier|ff-reminder-notifier-canary|vidura-runtime-listing|hulk-consumer|sentinel|dash-data-service|dash-picker-front|dash-cart|hulk-consumer-canary|sentinel-canary|dash-data-service-canary|dash-picker-front-canary|dash-cart-canary|surff|surff-canary|widget-manager|widget-manager-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|one-compass-service|one-compass-service-canary|food-pre-delivery-bot|food-pre-delivery-bot-canary|dash-serviceability|dash-serviceability-canary|offer-server|offer-server-canary|im-checkout-service|im-checkout-service-canary|vidura-runtime-dash|vidura-runtime-dash-canary|del-banner-go|del-banner-go-canary|unboxing-service|unboxing-service-canary|svcplatform-prediction|svcplatform-prediction-canary|svc-prediction|svc-prediction-canary|preorder-cod-eval|preorder-cod-eval-canary|vidura-runtime-menu|vidura-runtime-menu-canary|de-lifecycle-management|de-lifecycle-management-canary|seo-data|seo-data-canary|picker-assignment|picker-assignment-canary|postorder-abuse-eval|postorder-abuse-eval-canary|svcplatform-aggregator|svcplatform-aggregator-canary|order-platform|order-platform-canary|carousel-service|carousel-service-canary|aniket-service|aniket-service-canary|test-service|test-service-canary"})
      > 18000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: warning
  - alert: PrometheusMetricsDropped
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . The samples will be dropped by Prometheus as it has breached 20k sample
        limit.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"vidura-runtime-perf|frunvidura-runtime-listing-new|vidura-runtime-listing-new-canary|ff-reminder-notifier|ff-reminder-notifier-canary|vidura-runtime-listing|hulk-consumer|sentinel|dash-data-service|dash-picker-front|dash-cart|hulk-consumer-canary|sentinel-canary|dash-data-service-canary|dash-picker-front-canary|dash-cart-canary|surff|surff-canary|widget-manager|widget-manager-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|one-compass-service|one-compass-service-canary|food-pre-delivery-bot|food-pre-delivery-bot-canary|dash-serviceability|dash-serviceability-canary|offer-server|offer-server-canary|im-checkout-service|im-checkout-service-canary|vidura-runtime-dash|vidura-runtime-dash-canary|del-banner-go|del-banner-go-canary|unboxing-service|unboxing-service-canary|svcplatform-prediction|svcplatform-prediction-canary|svc-prediction|svc-prediction-canary|preorder-cod-eval|preorder-cod-eval-canary|vidura-runtime-menu|vidura-runtime-menu-canary|de-lifecycle-management|de-lifecycle-management-canary|seo-data|seo-data-canary|picker-assignment|picker-assignment-canary|postorder-abuse-eval|postorder-abuse-eval-canary|svcplatform-aggregator|svcplatform-aggregator-canary|order-platform|order-platform-canary|carousel-service|carousel-service-canary|aniket-service|aniket-service-canary|test-service|test-service-canary"})
      > 20000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsCritical
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . Metrics will be dropped post 50k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"collections-service|collections-service-canary|ie-pushgateway|cdc-prod-pg-cdc-prod-pg|cdc-snowflake-sync|ie-pushgateway-canary|cdc-prod-pg-cdc-prod-pg-canary|cdc-snowflake-sync-canary|surff|surff-canary|widget-manager|widget-manager-canary|cerebro|cerebro-canary|cerebro|cerebro-canary|core-discounting-server|core-discounting-server-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|vidura-runtime-ads|vidura-runtime-ads-canary|de-allocator|de-allocator-canary|checkout-service|confluent-lag-exporter|cerebro-listing|ads-serving-layer-rest|ads-serving-layer|ingestion-pipeline|banner|confluent-lag-exporter-canary|cerebro-listing-canary|ads-serving-layer-rest-canary|ads-serving-layer-canary|ingestion-pipeline-canary|banner-canary|del-banner-go|del-banner-go-canary|cerebro|carousel-service|carousel-service-canary|gandalf|gandalf-canary"})
      > 48000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusScrapeLimitsWarning
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}.
        Metrics will be dropped post 50k sample count.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"collections-service|collections-service-canary|ie-pushgateway|cdc-prod-pg-cdc-prod-pg|cdc-snowflake-sync|ie-pushgateway-canary|cdc-prod-pg-cdc-prod-pg-canary|cdc-snowflake-sync-canary|surff|surff-canary|widget-manager|widget-manager-canary|cerebro|cerebro-canary|cerebro|cerebro-canary|core-discounting-server|core-discounting-server-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|vidura-runtime-ads|vidura-runtime-ads-canary|de-allocator|de-allocator-canary|checkout-service|confluent-lag-exporter|cerebro-listing|ads-serving-layer-rest|ads-serving-layer|ingestion-pipeline|banner|confluent-lag-exporter-canary|cerebro-listing-canary|ads-serving-layer-rest-canary|ads-serving-layer-canary|ingestion-pipeline-canary|banner-canary|del-banner-go|del-banner-go-canary|cerebro|carousel-service|carousel-service-canary|gandalf|gandalf-canary"})
      > 45000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: warning
  - alert: PrometheusMetricsDropped
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . The samples will be dropped by Prometheus as it has breached 50k sample
        limit.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"collections-service|collections-service-canary|ie-pushgateway|cdc-prod-pg-cdc-prod-pg|cdc-snowflake-sync|ie-pushgateway-canary|cdc-prod-pg-cdc-prod-pg-canary|cdc-snowflake-sync-canary|surff|surff-canary|widget-manager|widget-manager-canary|cerebro|cerebro-canary|core-discounting-server|core-discounting-server-canary|ads-serving-layer-rest|ads-serving-layer-rest-canary|ads-serving-layer|ads-serving-layer-canary|vidura-runtime-ads|vidura-runtime-ads-canary|de-allocator|de-allocator-canary|checkout-service|confluent-lag-exporter|cerebro-listing|ads-serving-layer-rest|ads-serving-layer|ingestion-pipeline|banner|confluent-lag-exporter-canary|cerebro-listing-canary|ads-serving-layer-rest-canary|ads-serving-layer-canary|ingestion-pipeline-canary|banner-canary|del-banner-go|del-banner-go-canary|cerebro|carousel-service|carousel-service-canary|gandalf|gandalf-canary"})
      > 50000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: PrometheusMetricsDropped_80k
    annotations:
      description: Scrape sample count is {{ $value }} in container {{$labels.instance}}
        . The samples will be dropped by Prometheus as it has breached 80k sample
        limit.
    expr: 'sum by (service,instance)(scrape_samples_post_metric_relabeling{namespace=~"production|preprod",service=~"auto-assign|auto-assign-canary|carousel-service-nav|carousel-service-nav-canary|collections-service|collections-service-canary|carousel-service-nav|carousel-service-nav-canary|dash-business-metrics-service|dash-business-metrics-service-canary"})
      > 80000

      '
    for: 10m
    labels:
      app: '{{$labels.service}}'
      env: '{{$labels.namespace}}'
      pod: '{{$labels.instance}}'
      severity: critical
  - alert: ContainerDiskUsageWarning
    annotations:
      description: Container disk usage is greater than 40GB, please do not write
        logs to disk.
    expr: 'sum by (pod,cluster)(container_fs_usage_bytes{pod=~".+",pod!~"prometheus.*",job="kubelet"})
      > 40000000000

      '
    for: 10m
    labels:
      app: '{{$labels.pod}}'
      env: '{{$labels.namespace}}'
      severity: warning
  - alert: ContainerDiskUsageCritical
    annotations:
      description: Container disk usage is greater than 50GB,please do not write logs
        to disk.
    expr: 'sum by (pod,cluster)(container_fs_usage_bytes{pod=~".+",pod!~"prometheus.*",job="kubelet"})
      > 50000000000

      '
    for: 10m
    labels:
      app: '{{$labels.pod}}'
      env: '{{$labels.namespace}}'
      severity: critical
  - alert: LogmanLogsIngestionHighWarning
    annotations:
      dashboad_url: https://logman.swiggyops.de/d/YA5IR7qGz/rock-operations?orgId=1&viewPanel=24
      description: Logs generated by {{$labels.app}} is > 1TB/day. Logs for this service
        will be moved to S3. Please reduce the logs < 500GB/day to onboard to Logman.
    expr: 'sum(increase(promtail_custom_log_bytes_total{app!~"istio-gateway.*|offer-server.*|core-discounting-server.*"}[1d]
      offset 1m)) by (app) > 1099511627776

      '
    for: 10m
    labels:
      app: '{{$labels.app}}'
      env: '{{$labels.namespace}}'
      severity: warning
  - alert: LogmanLogsIngestionHighWarning-offer-server
    annotations:
      dashboad_url: https://logman.swiggyops.de/d/YA5IR7qGz/rock-operations?orgId=1&viewPanel=24
      description: Logs generated by {{$labels.app}} is > 1.5TB/day. Logs for this
        service will be moved to S3. Please reduce the logs < 500GB/day to onboard
        to Logman.
    expr: 'sum(increase(promtail_custom_log_bytes_total{app=~"offer-server.*|core-discounting-server.*"}[1d]
      offset 1m)) by (app) > 1649267441664

      '
    for: 10m
    labels:
      app: '{{$labels.app}}'
      env: '{{$labels.namespace}}'
      severity: warning
  - alert: ContainerThreadCountHigh
    annotations:
      description: Thread count by {{$labels.app}} service is > 20k for container
        {{$labels.pod}}.
    expr: 'sum by (pod,cluster,container)(container_threads{namespace=~"production|preprod",job="kubelet",node=~".+"})
      > 20000

      '
    for: 10m
    labels:
      app: '{{$labels.container}}'
      env: '{{$labels.namespace}}'
      severity: critical
  - alert: NodeThreadCountHigh
    annotations:
      description: Thread count by {{$labels.node}} node is > 16k.
    expr: 'sum by (node,cluster)(container_threads{job="kubelet",namespace!=""}) >
      160000

      '
    for: 10m
    labels:
      app: rock
      env: '{{$labels.namespace}}'
      severity: critical
  - alert: Core DNS POD On Same Node
    annotations:
      description: Two Core DNS POD {{$labels.pod}} Scheduled on Node {{$labels.node}}
    expr: 'count by (node, pod) (kube_pod_info{pod=~"coredns.*",cluster!="logger"})
      > 1

      '
    for: 5m
    labels:
      app: rock
      env: '{{$labels.namespace}}'
      severity: critical
